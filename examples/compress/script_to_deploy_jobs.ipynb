{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea6579e-cad8-40aa-9f04-388abf9023a8",
   "metadata": {},
   "source": [
    "# Notebook that runs thinning experiments on Slurm cluster or locally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e1e960-caf6-463f-a38d-5aa57f279c1d",
   "metadata": {},
   "source": [
    "## 1. Specify whether jobs will be run on Slurm cluster or locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aeba2a07-b66e-4633-aae8-23ca88a5a29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "deploy_slurm = True\n",
    "\n",
    "import subprocess # to use subprocess \n",
    "import pathlib\n",
    "import os\n",
    "import os.path\n",
    "import pickle as pkl\n",
    "from datetime import datetime\n",
    "if deploy_slurm:\n",
    "    from slurmpy import Slurm\n",
    "import numpy as np\n",
    "from goodpoints.util import isnotebook # Check whether this file is being executed as a script or as a notebook\n",
    "if isnotebook():\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e7b29-c29e-4d8d-b1d2-e2fbd96a45b5",
   "metadata": {},
   "source": [
    "## 2. Functions for generating python commands, and deploying jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "028e682c-2331-4c73-9388-4916ffb7fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_python_command(d, m, sz, rep0, repn, rerun, compute_mmd, recompute_mmd, log_folder,\n",
    "                       symm1=1, rh2 = 0,\n",
    "                   alg = \"kt\", compress_alg=None, n_compress=None, g=None,\n",
    "                       setting = \"gauss\", M=None, filename=None,\n",
    "                       results_folder = None, mcmc_folder = None,\n",
    "                       timing_experiment=False, \n",
    "                         ):\n",
    "    '''\n",
    "    return python string deploying an experiment\n",
    "    Args:\n",
    "        d: (int) dimension of the problem\n",
    "        m: (int) thinning factor in log_2 base (useful typically only for kt.thin)\n",
    "        sz: (int) the size of input data in log_4 base \n",
    "        rep0: (int) starting rep id\n",
    "        repn: (int) number of reps\n",
    "        rerun: (int) whether to rerun experiments when set to anything but 0, else 0\n",
    "        compute_mmd: (int) whether to compute mmd when set to anything but 0, else 0\n",
    "        recompute_mmd: (bool) whether to recompute mmd (i.e., ignore pkl file if mmd already exists) when set to anything but 0, else 0\n",
    "        log_folder: (str) folder for saving out/err files (useful only when deplying jobs via terminal, and not for slurm)\n",
    "        symm1: (int) whether to use symmetrize in stage 1 of compress++ when set to anything but 0, else 0\n",
    "        alg: (str) name of the main thinning algorithm\n",
    "        compress_alg: (int) the algorithm to be used for thinning in compress\n",
    "        n_compress: (int) number of compress coresets to be generated\n",
    "        g: (int) the oversampling factor \"g\" for compress++ experiments\n",
    "        setting: (str) target P; take values in gauss/mog/mcmc\n",
    "        M: (int) number of mog components\n",
    "        filename: (str) name of mcmc file\n",
    "        results_folder: (str) location for saving results\n",
    "        mcmc_folder : (str) location from where to load the mcmc data\n",
    "        timing_experiment: (bool) whether this experiment is about computing runtime only\n",
    "    \n",
    "    '''\n",
    "    assert(results_folder is not None)\n",
    "    pathlib.Path(results_folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if not timing_experiment:\n",
    "        exp_name =  ['python3', f'construct_{alg}_coresets.py']\n",
    "    else:\n",
    "        exp_name =  ['python3', f'run_time.py']\n",
    "    \n",
    "    exp_name.extend(['--m', f'{m}', f'--size', f'{sz}', '--setting', f'{setting}', '--rep0', f'{rep0}', \n",
    "                     '--repn', f'{repn}','--rerun', str(rerun), '--computemmd', str(compute_mmd), \n",
    "                     '--recomputemmd', str(recompute_mmd), '--symm1', str(symm1), \n",
    "                     '--resultsfolder', f'{results_folder}'\n",
    "                    ])\n",
    "    \n",
    "    if alg == \"compresspp\":\n",
    "        assert(compress_alg is not None)\n",
    "        exp_name.extend(['--compressalg', f'{compress_alg}']) \n",
    "        assert(g is not None)\n",
    "        exp_name.extend(['--g', f'{g}']) \n",
    "         \n",
    "    if setting == \"mog\":\n",
    "        exp_name.extend(['--M', f'{M}'])\n",
    "    \n",
    "    if setting != \"mcmc\":\n",
    "        exp_name.extend(['--d', f'{d}'])# add dimension for non mcmc settings\n",
    "    else:\n",
    "        exp_name.extend(['--filename', f'{filename}'])\n",
    "        assert(mcmc_folder is not None)\n",
    "        exp_name.extend(['--mcmcfolder', f'{mcmc_folder}'])\n",
    "        \n",
    "       \n",
    "\n",
    "    \n",
    "    log_file = ''.join(exp_name[1:])\n",
    "    # removing redundant characters to save space\n",
    "    log_file = log_file.replace(\"--\", \"-\")\n",
    "    for s in [\"construct_\", \"_coresets\", \".py\"]:\n",
    "        log_file = log_file.replace(s, \"\")\n",
    "    # adding time stamp\n",
    "    suffix = datetime.now().strftime('%H_%M')\n",
    "    out_file = os.path.join(log_folder, log_file+suffix+\".out\")\n",
    "    err_file = os.path.join(log_folder, log_file+suffix+\".err\")\n",
    "    \n",
    "    return(exp_name, out_file, err_file)\n",
    "\n",
    "def deploy_terminal_run(exp_name, out, err):\n",
    "    '''\n",
    "    deploy a python job as a python subprocess on terminal\n",
    "    \n",
    "    Args:\n",
    "        exp_name: (str) command for the job\n",
    "        out: (str) filename to save std_out\n",
    "        err: (str) filename to save std_err\n",
    "    '''\n",
    "    with open(out, \"wb\") as f:\n",
    "        with open(err, \"wb\") as f2:\n",
    "            subprocess.Popen(exp_name, stdout=f, stderr=f2)\n",
    "        return\n",
    "    \n",
    "def deploy_slurm_run(exp_name, partition, prefix):\n",
    "    '''\n",
    "    deploy a python job as a slurm job\n",
    "    \n",
    "    Args:\n",
    "        exp_name: (str) command for the job\n",
    "        partition: (str) partion name on the cluster\n",
    "        prefix: (str) prefix for the slurm job that is displayed in squeue\n",
    "    '''\n",
    "    if partition != \"jsteinhardt\":\n",
    "        s = Slurm(prefix, {\"partition\": partition,\n",
    "                     \"c\": 1\n",
    "                       \n",
    "                    })\n",
    "    else:\n",
    "        \n",
    "        s = Slurm(prefix, {\"partition\": partition,  })\n",
    "    s.run('module load python; ' + \" \".join(exp_name))\n",
    "    return(s)\n",
    "\n",
    "def deploy_experiment(deploy_slurm, ids, exp_name, out, err, partition, prefix, debug=False):\n",
    "    '''\n",
    "    deploy a python experiment via terminal or slurm and append the job id / experiment name to ids\n",
    "    \n",
    "    Args:\n",
    "        deploy_slurm: (Bool) if True deploy a slurm job, else a terminal job\n",
    "        ids: (list) list of deployed experiments\n",
    "        exp_name: (str) command for the job\n",
    "        out: (str) filename to save std_out when deploying a terminal job\n",
    "        err: (str) filename to save std_err when deploying a terminal job\n",
    "        partition: (str) partion name on the cluster when deploying a slurm job\n",
    "        prefix: (str) prefix for the slurm job that is displayed in squeue when deploying a slurm job\n",
    "        debug: (bool) do not deploy when true, else deploy\n",
    "    '''\n",
    "    if debug:\n",
    "        ids.append(\" \".join(exp_name))\n",
    "        return\n",
    "    \n",
    "    if deploy_slurm:\n",
    "        ids.append(deploy_slurm_run(partition=partition, prefix=prefix, exp_name=exp_name))\n",
    "    else:\n",
    "        deploy_terminal_run(exp_name, out, err)\n",
    "        ids.append(exp_name)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a2506eac-dfd3-4ac2-aaef-fb606111151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_exp_setting(ds, total_reps, reps_per_job, alg, target, Ms=None, filenames=None, gs=None, ms=None):\n",
    "    '''\n",
    "    print out experiment setting\n",
    "    \n",
    "    Args:\n",
    "        ds: (list) the dimensionality of experiments\n",
    "        total_reps: (int) the number of reps\n",
    "        reps_per_job: (int) number of reps for each job\n",
    "        alg: (str) the algorithm\n",
    "        target: (str) target P\n",
    "        Ms: (list) range of M for Mog target\n",
    "        filenames: (list) list of mcmc settings\n",
    "        gs: (list) list of oversampling factors for Compress++\n",
    "        ms : (list) sizes\n",
    "    '''\n",
    "    if filenames is not None:\n",
    "        print(f'Running {alg} experiments, gs={gs}, for {target} target for settings = {filenames} in d = {ds}, '\n",
    "              + f'm = {ms}, total_reps = {total_reps},' +\n",
    "                 f' with {reps_per_job} reps per python call')\n",
    "    elif Ms is not None:\n",
    "        print(f'Running {alg} experiments, gs={gs}, for {target} target for M = {Ms} in d = {ds}, ' \n",
    "                + f'm = {ms}, total_reps = {total_reps},' +\n",
    "                 f' with {reps_per_job} reps per python call')\n",
    "    else:\n",
    "        print(f'Running {alg} experiments, gs={gs}, for {target} target for d = {ds}, ' \n",
    "                + f'm = {ms}, total_reps = {total_reps},' +\n",
    "                 f' with {reps_per_job} reps per python call')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "da4beff2-a6f0-4422-8625-c0860c0c411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = [\"high\", \"yugroup\", \"jsteinhardt\", \"low\"]\n",
    "results_folder = \"coresets_folder\"\n",
    "mcmc_folder = \"/accounts/projects/binyu/raaz.rsk/kernel_thinning/kernel_thinning_plus/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73285091-44cb-4ffd-bed0-a921f7c2b57d",
   "metadata": {},
   "source": [
    "## 3. Set experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ea8f85b8-1b7e-4bc0-b9fb-61e276b056e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coresets to generate\n",
    "\n",
    "st_coresets = False\n",
    "kt_coresets = False\n",
    "herding_coresets = False\n",
    "\n",
    "cpp_kt_coresets = True\n",
    "cpp_herding_coresets = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2f8513ad-0ced-4d1e-9c08-03dc18491835",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ASSUMING SIZE = 4^m; and final output = sqrt(SIZE) = 2^m #####\n",
    "ms = range(10, 10+1) # range of m\n",
    "gs = [0, 4] # oversampling factors; will ignore Compress++ run for g>m\n",
    "\n",
    "# define repetition\n",
    "total_reps = 10 # set this to the max number of repetitions\n",
    "reps_per_job = 1 # number of reps per python call\n",
    "\n",
    "symm1 = 1 # whether want use symmetrized halving in compress\n",
    "\n",
    "compute_mmd = 1 # whether to compute mmd\n",
    "\n",
    "### whether to regenerate coreset / recompute MMD / IMPORTANT NOTE DIFFERENT FLAGS ####\n",
    "### if we want to recompute mmds (for some reason) then we have to set that flag to True ##\n",
    "rerun = 0 # regenerate coresets?\n",
    "recompute_mmd = 0 # recompute mmd? works ONLY if compute_mmd is true in the first place\n",
    "\n",
    "### All experiments are run with Gauss(sigma) as k and Gauss(sigma/sqrt(2)) as krt ###\n",
    "run_gauss_experiments = True # run experiments with Gauss P\n",
    "ds = [100]\n",
    "\n",
    "run_mog_experiments = False # run experiments with MoG P\n",
    "Ms = [4, 6, 8] # supports, 3, 4, 6, 8\n",
    "\n",
    "run_mcmc_experiments = False # run experiments with MCMC P\n",
    "all_mcmc_filenames = ['Goodwin_RW', 'Goodwin_ADA-RW', \n",
    "'Goodwin_MALA', 'Goodwin_PRECOND-MALA', \n",
    "'Lotka_RW', 'Lotka_ADA-RW', \n",
    "'Lotka_MALA', 'Lotka_PRECOND-MALA',  \n",
    "     'Hinch_P_seed_1_temp_1_scaled_nosplit',\n",
    "     'Hinch_P_seed_2_temp_1_scaled_nosplit',\n",
    "     'Hinch_TP_seed_1_temp_8_scaled_nosplit', \n",
    "     'Hinch_TP_seed_2_temp_8_scaled_nosplit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d41a0-caef-451e-b06e-137d0bdcc061",
   "metadata": {},
   "source": [
    "## 4. Run Gauss Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2201b356-9e9e-4c52-a4e4-201d02dfb5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False # whether to actually deploy experiments (False), or just return a list of python commands to be deployed (True)\n",
    "partition = partitions[2] # \"high\", \"yugroup\", \"jsteinhardt\", \"low\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "879401ef-b83b-468d-8882-2b93d67d69a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running compresspp(kt) experiments, gs=[0, 4], for gauss target for d = [100], m = range(10, 11), total_reps = 10, with 1 reps per python call\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Submitted batch job 1364897'\n",
      "b'Submitted batch job 1364898'\n",
      "b'Submitted batch job 1364899'\n",
      "b'Submitted batch job 1364900'\n",
      "b'Submitted batch job 1364901'\n",
      "b'Submitted batch job 1364902'\n",
      "b'Submitted batch job 1364903'\n",
      "b'Submitted batch job 1364904'\n",
      "b'Submitted batch job 1364905'\n",
      "b'Submitted batch job 1364906'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsteinhardt partition; Number of processes/jobs:20 slurm:True, terminal: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Submitted batch job 1364907'\n",
      "b'Submitted batch job 1364908'\n",
      "b'Submitted batch job 1364909'\n",
      "b'Submitted batch job 1364910'\n",
      "b'Submitted batch job 1364911'\n",
      "b'Submitted batch job 1364912'\n",
      "b'Submitted batch job 1364913'\n",
      "b'Submitted batch job 1364914'\n",
      "b'Submitted batch job 1364915'\n",
      "b'Submitted batch job 1364916'\n"
     ]
    }
   ],
   "source": [
    "gauss_ids = []\n",
    "if run_gauss_experiments:\n",
    "    log_folder = f\"logs/{datetime.now().strftime('%b_%d_%Y')}\"\n",
    "    pathlib.Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    target=\"gauss\"\n",
    "    \n",
    "    # run st/kt/herding experiments\n",
    "    for flag, alg in zip([st_coresets, kt_coresets, herding_coresets], [\"st\", \"kt\", \"herding\"]):\n",
    "        if flag:\n",
    "            print_exp_setting(ds, total_reps, reps_per_job, alg=alg, target=target)\n",
    "            for d in ds:\n",
    "                for m in ms:\n",
    "                    for i in range(0, total_reps, reps_per_job):\n",
    "                        # experiment command, and filenames\n",
    "                        exp_name, out, err = get_python_command(d=d, m=m, sz=m, rep0=i, repn=reps_per_job, \n",
    "                                                  rerun=rerun, compute_mmd=compute_mmd, \n",
    "                                                  recompute_mmd=recompute_mmd, log_folder=log_folder,\n",
    "                                                  alg=alg, compress_alg=None, n_compress=None, g=None,\n",
    "                                                  setting=target, symm1=symm1, results_folder=results_folder)\n",
    "                        prefix = alg[:1] + target[:1] + f\"d{d}n{m}r{i}\"\n",
    "                        deploy_experiment(deploy_slurm, gauss_ids, exp_name, out, err, partition, prefix=prefix, debug=debug)\n",
    "\n",
    "    # compress++ experiments\n",
    "    for flag, compress_alg in zip([cpp_kt_coresets, cpp_herding_coresets], [\"kt\", \"herding\"]):\n",
    "        if flag:\n",
    "            alg = \"compresspp\"\n",
    "            print_exp_setting(ds, total_reps, reps_per_job, alg=alg+f\"({compress_alg})\", target=target, gs=gs, ms=ms)\n",
    "            for d in ds:\n",
    "                for m in ms:\n",
    "                    for g in gs:\n",
    "                        if g > m:\n",
    "                            continue # skip the loop\n",
    "                        for i in range(0, total_reps, reps_per_job):\n",
    "                            # experiment command, and filenames\n",
    "                            exp_name, out, err = get_python_command(d=d, m=m, sz=m, rep0=i, repn=reps_per_job, \n",
    "                                                  rerun=rerun, compute_mmd=compute_mmd, \n",
    "                                                  recompute_mmd=recompute_mmd, log_folder=log_folder,\n",
    "                                                  alg=alg, compress_alg=compress_alg,  n_compress=None, g=g,\n",
    "                                                setting=target, symm1=symm1, results_folder=results_folder)\n",
    "                            prefix = alg[:1] + str(g) + compress_alg[:1] + target[:1] + f\"d{d}n{m}r{i}\"\n",
    "                            deploy_experiment(deploy_slurm, gauss_ids, exp_name, out, err, partition, prefix=prefix, debug=debug)\n",
    "                                            \n",
    "                            \n",
    "    print(f'{partition} partition; Number of processes/jobs:{len(gauss_ids)} slurm:{deploy_slurm}, terminal: {not deploy_slurm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "994a037e-0486-4058-8670-166f2bb4b2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gauss_ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bb2d6-eba2-45b3-8e13-eb4897ff7a97",
   "metadata": {},
   "source": [
    "## 5. Run MOG experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fdf37bb7-0c5c-46ff-949a-af52cdf7fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = \"coresets_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ace5375d-ebcb-499c-bab5-d5e7b612aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "partition = partitions[1]  # \"high\", \"yugroup\", \"jsteinhardt\", \"low\"\n",
    "mog_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b5b77031-9c5e-4015-96c1-7a443f3c8f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_mog_experiments:\n",
    "    log_folder = f\"logs/{datetime.now().strftime('%b_%d_%Y')}\"\n",
    "    pathlib.Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    d = 2\n",
    "    target=\"mog\"\n",
    "    \n",
    "    # run st/kt/herding experiments\n",
    "    for flag, alg in zip([st_coresets, kt_coresets, herding_coresets], [\"st\", \"kt\", \"herding\"]):\n",
    "        if flag:\n",
    "            print_exp_setting(d, total_reps, reps_per_job, alg=alg, target=target, Ms=Ms)\n",
    "            for M in Ms:\n",
    "                for m in ms:\n",
    "                    for i in range(0, total_reps, reps_per_job):\n",
    "                        # experiment command, and filenames\n",
    "                        exp_name, out, err = get_python_command(d=d, m=m, sz=m, rep0=i, repn=reps_per_job, \n",
    "                                                  rerun=rerun, compute_mmd=compute_mmd, \n",
    "                                                  recompute_mmd=recompute_mmd, log_folder=log_folder,\n",
    "                                                  alg=alg, compress_alg=None, n_compress=None, g=None,\n",
    "                                                setting=target, M=M, symm1=symm1, results_folder=results_folder)\n",
    "                        prefix = alg[:1] + target[1] + f\"M{M}n{m}r{i}\" \n",
    "                        deploy_experiment(deploy_slurm, mog_ids, exp_name, out, err, partition, prefix=prefix, debug=debug)\n",
    "\n",
    "\n",
    "\n",
    "    # compress++ experiments\n",
    "    for flag, compress_alg in zip([cpp_kt_coresets, cpp_herding_coresets], [\"kt\", \"herding\"]):\n",
    "        if flag:\n",
    "            alg = \"compresspp\"\n",
    "            print_exp_setting(d, total_reps, reps_per_job, alg=alg+f\"({compress_alg})\", \n",
    "                              target=target, Ms=Ms, gs=gs)\n",
    "            for M in Ms:\n",
    "                for m in ms:\n",
    "                    for g in gs:\n",
    "                        if g > m:\n",
    "                            continue # skip the loop\n",
    "                        for i in range(0, total_reps, reps_per_job):\n",
    "                            # experiment command, and filenames\n",
    "                            exp_name, out, err = get_python_command(d=d, m=m, sz=m, rep0=i, repn=reps_per_job, \n",
    "                                                  rerun=rerun, compute_mmd=compute_mmd, \n",
    "                                                  recompute_mmd=recompute_mmd, log_folder=log_folder,\n",
    "                                                  alg=alg, compress_alg=compress_alg,  n_compress=None, \n",
    "                                                  g=g, setting=target, M=M, symm1=symm1, results_folder=results_folder)\n",
    "                            prefix = alg[:1] + str(g) + compress_alg[:1] +  target[1] + f\"M{M}n{m}r{i}\" \n",
    "                            deploy_experiment(deploy_slurm, mog_ids, exp_name, out, err, partition, prefix=prefix, debug=debug)\n",
    "                                \n",
    "                        \n",
    "    print(f'{partition} partition; Number of processes/jobs:{len(mog_ids)} slurm:{deploy_slurm}, terminal: {not deploy_slurm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093ab5c2-5490-4915-9ba4-086f2c01108b",
   "metadata": {},
   "source": [
    "## 6. Run MCMC experiments (we set parameters again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "78dceb71-dfe2-487e-8d0c-a6c799bebd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mcmc_experiments = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "04231201-37b5-47cf-ad94-ee0380af0eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coresets to generate\n",
    "\n",
    "st_coresets = True\n",
    "kt_coresets = False\n",
    "herding_coresets = False\n",
    "\n",
    "cpp_kt_coresets = False\n",
    "cpp_herding_coresets = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1c3dffb4-628d-416e-931c-27088c050c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = [0, 4] # oversampling factors; will ignore Compress++ run for g>m\n",
    "\n",
    "# define repetition\n",
    "total_reps = 10 # set this to the max number of repetitions\n",
    "reps_per_job = 1 # number of reps per python call\n",
    "\n",
    "symm1 = 1 # whether want use symmetrized halving in compress\n",
    "\n",
    "compute_mmd = 1 # whether to compute mmd\n",
    "\n",
    "### whether to regenerate coreset / recompute MMD / IMPORTANT NOTE DIFFERENT FLAGS ####\n",
    "### if we want to recompute mmds (for some reason) then we have to set that flag to True ##\n",
    "rerun = 0 # regenerate coresets?\n",
    "recompute_mmd = 0 # recompute mmd? works ONLY if compute_mmd is true in the first place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2a46d917-85d1-4a5d-bf81-b9e12fd709de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hinch_P_seed_1_temp_1_scaled_nosplit', 'Hinch_P_seed_2_temp_1_scaled_nosplit', 'Hinch_TP_seed_1_temp_8_scaled_nosplit', 'Hinch_TP_seed_2_temp_8_scaled_nosplit']\n"
     ]
    }
   ],
   "source": [
    "mcmc_file_range = slice(8, 12)\n",
    "mcmc_filenames = all_mcmc_filenames[mcmc_file_range]\n",
    "print(mcmc_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f0960f4f-60c5-4402-9755-76640e04fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the range of allowed ms is 4^4 to 4^8 for all Hinch experiments and Lotka_ADA-RW;\n",
    "# the range of allowed ms is 4^4 to 4^9 for all other experiments;\n",
    "\n",
    "min_m = 4 # to cap the min size of experiments regardless of allowed ms;\n",
    "max_m = 10 # to cap the max size of experiments regardless of allowed ms;\n",
    "# the range is truncted to min(max_allowed_range, max_m)\n",
    "\n",
    "ms_ranges = dict()\n",
    "for filename in all_mcmc_filenames:\n",
    "    if ('Hinch' in filename) or (filename == 'Lotka_ADA-RW'):\n",
    "        ms_ranges[filename] = range(max(4, min_m), min(8, max_m)+1)\n",
    "    else:\n",
    "        ms_ranges[filename] = range(max(4, min_m), min(9, max_m)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9938d466-4051-4735-9044-4dd17219cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "partition = partitions[3] # \"high\", \"yugroup\", \"jsteinhardt\", \"low\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0de2f867-02a9-4134-b7ee-6972b7104893",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running st experiments, gs=None, for mcmc target for settings = ['Hinch_P_seed_1_temp_1_scaled_nosplit', 'Hinch_P_seed_2_temp_1_scaled_nosplit', 'Hinch_TP_seed_1_temp_8_scaled_nosplit', 'Hinch_TP_seed_2_temp_8_scaled_nosplit'] in d = 0, m = {'Goodwin_RW': range(4, 10), 'Goodwin_ADA-RW': range(4, 10), 'Goodwin_MALA': range(4, 10), 'Goodwin_PRECOND-MALA': range(4, 10), 'Lotka_RW': range(4, 10), 'Lotka_ADA-RW': range(4, 9), 'Lotka_MALA': range(4, 10), 'Lotka_PRECOND-MALA': range(4, 10), 'Hinch_P_seed_1_temp_1_scaled_nosplit': range(4, 9), 'Hinch_P_seed_2_temp_1_scaled_nosplit': range(4, 9), 'Hinch_TP_seed_1_temp_8_scaled_nosplit': range(4, 9), 'Hinch_TP_seed_2_temp_8_scaled_nosplit': range(4, 9)}, total_reps = 10, with 1 reps per python call\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Submitted batch job 1364388'\n",
      "b'Submitted batch job 1364389'\n",
      "b'Submitted batch job 1364390'\n",
      "b'Submitted batch job 1364391'\n",
      "b'Submitted batch job 1364392'\n",
      "b'Submitted batch job 1364393'\n",
      "b'Submitted batch job 1364394'\n",
      "b'Submitted batch job 1364395'\n",
      "b'Submitted batch job 1364396'\n",
      "b'Submitted batch job 1364397'\n",
      "b'Submitted batch job 1364398'\n",
      "b'Submitted batch job 1364399'\n",
      "b'Submitted batch job 1364400'\n",
      "b'Submitted batch job 1364401'\n",
      "b'Submitted batch job 1364402'\n",
      "b'Submitted batch job 1364403'\n",
      "b'Submitted batch job 1364404'\n",
      "b'Submitted batch job 1364405'\n",
      "b'Submitted batch job 1364406'\n",
      "b'Submitted batch job 1364407'\n",
      "b'Submitted batch job 1364408'\n",
      "b'Submitted batch job 1364409'\n",
      "b'Submitted batch job 1364410'\n",
      "b'Submitted batch job 1364411'\n",
      "b'Submitted batch job 1364412'\n",
      "b'Submitted batch job 1364413'\n",
      "b'Submitted batch job 1364414'\n",
      "b'Submitted batch job 1364415'\n",
      "b'Submitted batch job 1364416'\n",
      "b'Submitted batch job 1364417'\n",
      "b'Submitted batch job 1364418'\n",
      "b'Submitted batch job 1364419'\n",
      "b'Submitted batch job 1364420'\n",
      "b'Submitted batch job 1364421'\n",
      "b'Submitted batch job 1364422'\n",
      "b'Submitted batch job 1364423'\n",
      "b'Submitted batch job 1364424'\n",
      "b'Submitted batch job 1364425'\n",
      "b'Submitted batch job 1364426'\n",
      "b'Submitted batch job 1364427'\n",
      "b'Submitted batch job 1364428'\n",
      "b'Submitted batch job 1364429'\n",
      "b'Submitted batch job 1364430'\n",
      "b'Submitted batch job 1364431'\n",
      "b'Submitted batch job 1364432'\n",
      "b'Submitted batch job 1364433'\n",
      "b'Submitted batch job 1364434'\n",
      "b'Submitted batch job 1364435'\n",
      "b'Submitted batch job 1364436'\n",
      "b'Submitted batch job 1364437'\n",
      "b'Submitted batch job 1364438'\n",
      "b'Submitted batch job 1364439'\n",
      "b'Submitted batch job 1364440'\n",
      "b'Submitted batch job 1364441'\n",
      "b'Submitted batch job 1364442'\n",
      "b'Submitted batch job 1364443'\n",
      "b'Submitted batch job 1364444'\n",
      "b'Submitted batch job 1364445'\n",
      "b'Submitted batch job 1364446'\n",
      "b'Submitted batch job 1364447'\n",
      "b'Submitted batch job 1364448'\n",
      "b'Submitted batch job 1364449'\n",
      "b'Submitted batch job 1364450'\n",
      "b'Submitted batch job 1364451'\n",
      "b'Submitted batch job 1364452'\n",
      "b'Submitted batch job 1364453'\n",
      "b'Submitted batch job 1364454'\n",
      "b'Submitted batch job 1364455'\n",
      "b'Submitted batch job 1364456'\n",
      "b'Submitted batch job 1364457'\n",
      "b'Submitted batch job 1364458'\n",
      "b'Submitted batch job 1364459'\n",
      "b'Submitted batch job 1364460'\n",
      "b'Submitted batch job 1364461'\n",
      "b'Submitted batch job 1364462'\n",
      "b'Submitted batch job 1364463'\n",
      "b'Submitted batch job 1364464'\n",
      "b'Submitted batch job 1364465'\n",
      "b'Submitted batch job 1364466'\n",
      "b'Submitted batch job 1364467'\n",
      "b'Submitted batch job 1364468'\n",
      "b'Submitted batch job 1364469'\n",
      "b'Submitted batch job 1364470'\n",
      "b'Submitted batch job 1364471'\n",
      "b'Submitted batch job 1364472'\n",
      "b'Submitted batch job 1364473'\n",
      "b'Submitted batch job 1364474'\n",
      "b'Submitted batch job 1364475'\n",
      "b'Submitted batch job 1364476'\n",
      "b'Submitted batch job 1364477'\n",
      "b'Submitted batch job 1364478'\n",
      "b'Submitted batch job 1364479'\n",
      "b'Submitted batch job 1364480'\n",
      "b'Submitted batch job 1364481'\n",
      "b'Submitted batch job 1364482'\n",
      "b'Submitted batch job 1364483'\n",
      "b'Submitted batch job 1364484'\n",
      "b'Submitted batch job 1364485'\n",
      "b'Submitted batch job 1364486'\n",
      "b'Submitted batch job 1364487'\n",
      "b'Submitted batch job 1364488'\n",
      "b'Submitted batch job 1364489'\n",
      "b'Submitted batch job 1364490'\n",
      "b'Submitted batch job 1364491'\n",
      "b'Submitted batch job 1364492'\n",
      "b'Submitted batch job 1364493'\n",
      "b'Submitted batch job 1364494'\n",
      "b'Submitted batch job 1364495'\n",
      "b'Submitted batch job 1364496'\n",
      "b'Submitted batch job 1364497'\n",
      "b'Submitted batch job 1364498'\n",
      "b'Submitted batch job 1364499'\n",
      "b'Submitted batch job 1364500'\n",
      "b'Submitted batch job 1364501'\n",
      "b'Submitted batch job 1364502'\n",
      "b'Submitted batch job 1364503'\n",
      "b'Submitted batch job 1364504'\n",
      "b'Submitted batch job 1364505'\n",
      "b'Submitted batch job 1364506'\n",
      "b'Submitted batch job 1364507'\n",
      "b'Submitted batch job 1364508'\n",
      "b'Submitted batch job 1364509'\n",
      "b'Submitted batch job 1364510'\n",
      "b'Submitted batch job 1364511'\n",
      "b'Submitted batch job 1364512'\n",
      "b'Submitted batch job 1364513'\n",
      "b'Submitted batch job 1364514'\n",
      "b'Submitted batch job 1364515'\n",
      "b'Submitted batch job 1364516'\n",
      "b'Submitted batch job 1364517'\n",
      "b'Submitted batch job 1364518'\n",
      "b'Submitted batch job 1364519'\n",
      "b'Submitted batch job 1364520'\n",
      "b'Submitted batch job 1364521'\n",
      "b'Submitted batch job 1364522'\n",
      "b'Submitted batch job 1364523'\n",
      "b'Submitted batch job 1364524'\n",
      "b'Submitted batch job 1364525'\n",
      "b'Submitted batch job 1364526'\n",
      "b'Submitted batch job 1364527'\n",
      "b'Submitted batch job 1364528'\n",
      "b'Submitted batch job 1364529'\n",
      "b'Submitted batch job 1364530'\n",
      "b'Submitted batch job 1364531'\n",
      "b'Submitted batch job 1364532'\n",
      "b'Submitted batch job 1364533'\n",
      "b'Submitted batch job 1364534'\n",
      "b'Submitted batch job 1364535'\n",
      "b'Submitted batch job 1364536'\n",
      "b'Submitted batch job 1364537'\n",
      "b'Submitted batch job 1364538'\n",
      "b'Submitted batch job 1364539'\n",
      "b'Submitted batch job 1364540'\n",
      "b'Submitted batch job 1364541'\n",
      "b'Submitted batch job 1364542'\n",
      "b'Submitted batch job 1364543'\n",
      "b'Submitted batch job 1364544'\n",
      "b'Submitted batch job 1364545'\n",
      "b'Submitted batch job 1364546'\n",
      "b'Submitted batch job 1364547'\n",
      "b'Submitted batch job 1364548'\n",
      "b'Submitted batch job 1364549'\n",
      "b'Submitted batch job 1364550'\n",
      "b'Submitted batch job 1364551'\n",
      "b'Submitted batch job 1364552'\n",
      "b'Submitted batch job 1364553'\n",
      "b'Submitted batch job 1364554'\n",
      "b'Submitted batch job 1364555'\n",
      "b'Submitted batch job 1364556'\n",
      "b'Submitted batch job 1364557'\n",
      "b'Submitted batch job 1364558'\n",
      "b'Submitted batch job 1364559'\n",
      "b'Submitted batch job 1364560'\n",
      "b'Submitted batch job 1364561'\n",
      "b'Submitted batch job 1364562'\n",
      "b'Submitted batch job 1364563'\n",
      "b'Submitted batch job 1364564'\n",
      "b'Submitted batch job 1364565'\n",
      "b'Submitted batch job 1364566'\n",
      "b'Submitted batch job 1364567'\n",
      "b'Submitted batch job 1364568'\n",
      "b'Submitted batch job 1364569'\n",
      "b'Submitted batch job 1364570'\n",
      "b'Submitted batch job 1364571'\n",
      "b'Submitted batch job 1364572'\n",
      "b'Submitted batch job 1364573'\n",
      "b'Submitted batch job 1364574'\n",
      "b'Submitted batch job 1364575'\n",
      "b'Submitted batch job 1364576'\n",
      "b'Submitted batch job 1364577'\n",
      "b'Submitted batch job 1364578'\n",
      "b'Submitted batch job 1364579'\n",
      "b'Submitted batch job 1364580'\n",
      "b'Submitted batch job 1364581'\n",
      "b'Submitted batch job 1364582'\n",
      "b'Submitted batch job 1364583'\n",
      "b'Submitted batch job 1364584'\n",
      "b'Submitted batch job 1364585'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low partition; Number of processes/jobs:200 slurm:True, terminal: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Submitted batch job 1364586'\n",
      "b'Submitted batch job 1364587'\n"
     ]
    }
   ],
   "source": [
    "mcmc_ids = []\n",
    "if run_mcmc_experiments:\n",
    "    log_folder = f\"logs/{datetime.now().strftime('%b_%d_%Y')}\"\n",
    "    pathlib.Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    d = 0 # None # no fixed d\n",
    "    target=\"mcmc\"\n",
    "    \n",
    "    # run st/kt/herding experiments\n",
    "    for flag, alg in zip([st_coresets, kt_coresets, herding_coresets], [\"st\", \"kt\", \"herding\"]):\n",
    "        if flag:\n",
    "            print_exp_setting(d, total_reps, reps_per_job, alg=alg, target=target, Ms=None, filenames=mcmc_filenames, ms=ms_ranges)\n",
    "            for filename in mcmc_filenames:\n",
    "                for m in ms_ranges[filename]:\n",
    "                    for i in range(0, total_reps, reps_per_job):\n",
    "                        # experiment command, and filenames\n",
    "                        exp_name, out, err = get_python_command(d=d, m=m, sz=m, rep0=i, repn=reps_per_job, \n",
    "                                                  rerun=rerun, compute_mmd=compute_mmd, \n",
    "                                                  recompute_mmd=recompute_mmd, log_folder=log_folder,\n",
    "                                                  alg=alg, compress_alg=None, n_compress=None, g=None,\n",
    "                                                setting=target, filename=filename, symm1=symm1, results_folder=results_folder, mcmc_folder = mcmc_folder)\n",
    "                        prefix = alg[:1] + filename[:2] + f\"d{d}n{m}r{i}\" \n",
    "                        deploy_experiment(deploy_slurm, mcmc_ids, exp_name, out, err, partition, prefix=prefix, debug=debug)\n",
    "                        \n",
    "    # compress++ experiments\n",
    "    for flag, compress_alg in zip([cpp_kt_coresets, cpp_herding_coresets], [\"kt\", \"herding\"]):\n",
    "        if flag:\n",
    "            alg = \"compresspp\"\n",
    "            print_exp_setting(d, total_reps, reps_per_job, alg=alg+f\"({compress_alg})\", \n",
    "                              target=target, Ms=None,filenames=mcmc_filenames, gs=gs, ms=ms_ranges)\n",
    "            for filename in mcmc_filenames:\n",
    "                for m in ms_ranges[filename]:\n",
    "                    for g in gs:\n",
    "                        if g > m:\n",
    "                            continue\n",
    "                        for i in range(0, total_reps, reps_per_job):\n",
    "                            # experiment command, and filenames\n",
    "                            exp_name, out, err = get_python_command(d=d, m=m, sz=m, rep0=i, repn=reps_per_job, \n",
    "                                                  rerun=rerun, compute_mmd=compute_mmd, \n",
    "                                                  recompute_mmd=recompute_mmd, log_folder=log_folder,\n",
    "                                                  alg=alg, compress_alg=compress_alg,  n_compress=None, \n",
    "                                                  g=g, setting=target, filename=filename, symm1=symm1, results_folder=results_folder, mcmc_folder = mcmc_folder)\n",
    "                            prefix = alg[:1] + compress_alg[:1]  + filename[:1] + filename[5:6] + f\"n{m}r{i}\" \n",
    "                            deploy_experiment(deploy_slurm, mcmc_ids, exp_name, out, err, partition, prefix=prefix, debug=debug)\n",
    "                                \n",
    "    print(f'{partition} partition; Number of processes/jobs:{len(mcmc_ids)} slurm:{deploy_slurm}, terminal: {not deploy_slurm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f764a3-e592-4c7c-940b-0d17d37ca536",
   "metadata": {},
   "source": [
    "## 7. Runtime experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4194f2fc-86de-4edc-8cfe-3ce025767313",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = \"results/run_time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad969bd0-8a4c-4de1-a972-d4b72360fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coresets to generate\n",
    "\n",
    "st_coresets = False\n",
    "kt_coresets = True\n",
    "herding_coresets = False\n",
    "\n",
    "cpp_kt_coresets = False\n",
    "cpp_herding_coresets = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b7e1a63-781e-457e-82e5-805f6140c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ASSUMING SIZE = 4^m; and final output = sqrt(SIZE) = 2^m #####\n",
    "ms = range(4, 5+1) # range of m\n",
    "gs = [0, 4] # oversampling factors; will ignore C++ run for g>m\n",
    "\n",
    "# define repetition\n",
    "total_reps = 3 # set this to the max number of repetitions\n",
    "reps_per_job = 1 # number of reps per python call\n",
    "\n",
    "symm1 = 1 # whether want use symetric compress in stage 1;\n",
    "\n",
    "rerun = 0 # re generate coresets\n",
    "\n",
    "### All experiments are run with Gauss(sigma) as k and Gauss(sigma/sqrt(2)) as krt ###\n",
    "runtime_gauss_experiments = True # run experiments with Gauss P\n",
    "ds = [2] #, 4, 10, 100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "457a200f-bc7c-4a3e-b103-177133aeef75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running kt experiments, alphas=[0, 4], for gauss target for d = [2], m = range(4, 6), total_reps = 3, with 1 reps per python call\n",
      "jsteinhardt partition; Number of processes/jobs:6 slurm:True, terminal: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Submitted batch job 1092447'\n",
      "b'Submitted batch job 1092448'\n",
      "b'Submitted batch job 1092449'\n",
      "b'Submitted batch job 1092450'\n",
      "b'Submitted batch job 1092451'\n",
      "b'Submitted batch job 1092452'\n"
     ]
    }
   ],
   "source": [
    "gauss_ids = []\n",
    "if runtime_gauss_experiments:\n",
    "    log_folder = f\"logs/{datetime.now().strftime('%b_%d_%Y')}\"\n",
    "    pathlib.Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    target=\"gauss\"\n",
    "    \n",
    "    # run st/kt/herding experiments\n",
    "    for flag, alg in zip([st_coresets, kt_coresets, herding_coresets], [\"st\", \"kt\", \"herding\"]):\n",
    "        if flag:\n",
    "            print_exp_setting(ds, total_reps, reps_per_job, alg=alg, target=target)\n",
    "            for d in ds:\n",
    "                for m in ms:\n",
    "                    for i in range(0, total_reps, reps_per_job):\n",
    "                        # experiment command, and filenames\n",
    "                        exp_name, out, err = get_python_command(d=d, m=m, sz=m, rep0=i, repn=reps_per_job, \n",
    "                                                  rerun=rerun, compute_mmd=compute_mmd, \n",
    "                                                  recompute_mmd=recompute_mmd, log_folder=log_folder,\n",
    "                                                  alg=alg, compress_alg=None, n_compress=None, g=None,\n",
    "                                                  setting=target,\n",
    "                                                symm1=symm1, timing_experiment=True, results_folder = results_folder)\n",
    "                        prefix = 'rt' + alg[:1] + target[:1] + f\"d{d}n{m}r{i}\"\n",
    "                        deploy_experiment(deploy_slurm, gauss_ids, exp_name, out, err, partition, prefix=prefix, debug=debug)\n",
    "\n",
    "    # compress++ experiments\n",
    "    for flag, compress_alg in zip([cpp_kt_coresets, cpp_herding_coresets], [\"kt\", \"herding\"]):\n",
    "        if flag:\n",
    "            alg = \"compresspp\"\n",
    "            print_exp_setting(ds, total_reps, reps_per_job, alg=alg+f\"({compress_alg})\", target=target, gs=gs)\n",
    "            for d in ds:\n",
    "                for m in ms:\n",
    "                    for g in gs:\n",
    "                        if g > m:\n",
    "                            continue # skip the loop\n",
    "                        for i in range(0, total_reps, reps_per_job):\n",
    "                            # experiment command, and filenames\n",
    "                            exp_name, out, err = get_python_command(d=d, m=m, sz=m, rep0=i, repn=reps_per_job, \n",
    "                                                  rerun=rerun, compute_mmd=compute_mmd, \n",
    "                                                  recompute_mmd=recompute_mmd, log_folder=log_folder,\n",
    "                                                  alg=alg, compress_alg=compress_alg,  n_compress=None, g=g,\n",
    "                                                setting=target,\n",
    "                                                symm1=symm1, timing_experiment=True, results_folder = results_folder)\n",
    "                            prefix = 'rt' + alg[:1] + str(g) + compress_alg[:1] + target[:1] + f\"d{d}n{m}r{i}\"\n",
    "                            deploy_experiment(deploy_slurm, gauss_ids, exp_name, out, err, partition, prefix=prefix, debug=debug)\n",
    "                                            \n",
    "                            \n",
    "    print(f'{partition} partition; Number of processes/jobs:{len(gauss_ids)} slurm:{deploy_slurm}, terminal: {not deploy_slurm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8621a-d441-446b-a268-11ec3f4bbc46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
